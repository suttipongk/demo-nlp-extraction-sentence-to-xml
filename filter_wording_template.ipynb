{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"0_Filter_wording_Template.ipynb","provenance":[{"file_id":"1v2Pr5HC8JFpm7EarC0EVDnxiRliQ7haj","timestamp":1575114088415}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"13wS1LI2xrpF","executionInfo":{"status":"ok","timestamp":1605424868174,"user_tz":-420,"elapsed":1997,"user":{"displayName":"Sut","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiqTtYUEpWQsymNl0gpoWkeQVgPIM3Wq21YCM51xtw=s64","userId":"11673919771648604082"}},"outputId":"e403740f-b797-4274-d0ba-8ca7d9eac60e","colab":{"base_uri":"https://localhost:8080/"}},"source":["#sentences = \"The incident management process start with the detection of a service failure\".split(\".\")\n","sentences = \"My name is sing song as well. My am a mother as well as tall. I am happy. You sing like my mother\".split(\".\")\n","search_keywords=['as','as']\n","\n","for sentence in sentences:\n","    print(\"{} key words in sentence:\".format(sum(1 for word in search_keywords if word in sentence)))\n","    print(sentence + \"\\n\")\n","\n","#===================================\n","dct = {}\n","for sentence in sentences:\n","    dct[sentence] = sum(1 for word in search_keywords if word in sentence)\n","best_sentences = [key for key,value in dct.items() if value == max(dct.values())]\n","print(\"\\n\".join(best_sentences))\n","\n","# Outputs:\n","#My name is sing song\n","# You sing like my mother\n","#====================================\n","if (any(map(lambda word: word in sentence, search_keywords))):\n","    print ('sentence :', sentence)\n","\n","#====================================\n","sent_words = []\n","for sentence in sentences:\n","    sent_words.append(set(sentence.split()))\n","num_keywords = [len(sent & set(search_keywords)) for sent in sent_words]\n","\n","# Find only one sentence\n","ind = num_keywords.index(max(num_keywords))\n","# Find all sentences with that number of keywords\n","ind = [i for i, x in enumerate(num_keywords) if x == max(num_keywords)]"],"execution_count":null,"outputs":[{"output_type":"stream","text":["2 key words in sentence:\n","My name is sing song as well\n","\n","2 key words in sentence:\n"," My am a mother as well as tall\n","\n","0 key words in sentence:\n"," I am happy\n","\n","0 key words in sentence:\n"," You sing like my mother\n","\n","My name is sing song as well\n"," My am a mother as well as tall\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"9ZpqC22jyB9c","executionInfo":{"status":"ok","timestamp":1605424918185,"user_tz":-420,"elapsed":1165,"user":{"displayName":"Sut","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiqTtYUEpWQsymNl0gpoWkeQVgPIM3Wq21YCM51xtw=s64","userId":"11673919771648604082"}},"outputId":"4a7b5616-a6c6-4808-cd87-139506d56731","colab":{"base_uri":"https://localhost:8080/"}},"source":["import re\n","\n","templates = [\n","    \"I have four {fruit} in {place}\",\n","    \"I have four {fruit} and {grain} in {place}\",\n","]\n","\n","my_dict = {'fruit': ['apple', 'banana', 'mango'],\n","           'place': ['kitchen', 'living room'],\n","           'grain' : ['wheat', 'rice']\n","           }\n","\n","words = {k: '(?P<{}>{})'.format(k, '|'.join(map(re.escape, v))) for k, v in my_dict.items()}\n","patterns = [re.compile(template.format(**words)) for template in templates]\n","\n","def find_matches(sentence):\n","    for pattern in patterns:\n","        match = pattern.match(sentence)\n","        if match:\n","            return match.groupdict()\n","\n","print(find_matches(\"I have four apple in kitchen\"))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["{'fruit': 'apple', 'place': 'kitchen'}\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"3RNNkzcHyJY7","executionInfo":{"status":"ok","timestamp":1605424955703,"user_tz":-420,"elapsed":4184,"user":{"displayName":"Sut","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiqTtYUEpWQsymNl0gpoWkeQVgPIM3Wq21YCM51xtw=s64","userId":"11673919771648604082"}},"outputId":"bd65aace-346c-4262-c2b1-35c9befa2f2b","colab":{"base_uri":"https://localhost:8080/"}},"source":["import os, sys\n","import spacy\n","from spacy.matcher import Matcher\n","\n","text = \"Receipt department returns the good to the vendor and system sends notification to the purchase department\"\n","\n","SUBJECTS = [\"nsubj\", \"nsubjpass\", \"csubj\", \"csubjpass\", \"agent\", \"expl\"]\n","VERB = [\"ROOT\"]\n","OBJECTS = [\"dobj\", \"dative\", \"attr\", \"oprd\"]\n","bpmn_label = []\n","\n","nlp = spacy.load(\"en_core_web_sm\")\n","doc = nlp(text)\n","\n","def isNegated(tok):\n","    negations = {\"no\", \"not\", \"n't\", \"never\", \"none\"}\n","    for dep in list(tok.lefts) + list(tok.rights):\n","        if dep.lower_ in negations:\n","            return True\n","    return False\n","\n","def getSubsFromConjunctions(subs):\n","    moreSubs = []\n","    for sub in subs:\n","        # rights is a generator\n","        rights = list(sub.rights)\n","        rightDeps = {tok.lower_ for tok in rights}\n","        if \"and\" in rightDeps:\n","            moreSubs.extend([tok for tok in rights if tok.dep_ in SUBJECTS or tok.pos_ == \"NOUN\"])\n","            if len(moreSubs) > 0:\n","                moreSubs.extend(getSubsFromConjunctions(moreSubs))\n","                #print('get more subjects', moreSubs)\n","    return moreSubs\n","\n","def findSubs(tok):\n","    head = tok.head\n","    pos = tok.pos_\n","    while head.pos_ != \"VERB\" and head.pos_ != \"NOUN\" and head.head != head:\n","        head = head.head\n","    if head.pos_ == \"VERB\":\n","        subs = [tok for tok in head.lefts if tok.dep_ in SUBJECTS]\n","        if len(subs) > 0:\n","            verbNegated = isNegated(head)\n","            subs.extend(getSubsFromConjunctions(subs))\n","            return subs, verbNegated\n","        elif head.head != head:\n","            return findSubs(head)\n","    elif head.pos_ == \"NOUN\":\n","        return [head], isNegated(tok)\n","    return [], False\n","\n","def findSubject_with_negated(tokens):\n","    subs = []\n","    verbs = [tok for tok in tokens if tok.pos_ == \"VERB\" and tok.dep_ != \"aux\"]\n","    for v in verbs:\n","        verbNegated = isNegated(v)\n","        subject = [tok for tok in tokens.lefts if tok.dep_ in SUBJECTS]\n","        \n","        #ถ้าหาไม่เจอ ให้ทำต่อ\n","        if len(subject) > 0:\n","            subs.extend(getSubsFromConjunctions(subject))\n","        else:\n","            foundSubs, verbNegated = findSubs(v)\n","            subs.extend(foundSubs)\n","    print('---SUBJECT---', subs)\n","\n","def findVerb(tokens):\n","    subject = [tok for tok in tokens if tok.dep_ in VERB]\n","    print(\"---VERB---\", subject)\n","\n","def findObject(tokens):\n","    verb = [tok for tok in tokens if tok.dep_ in OBJECTS]\n","    print(\"---OBJECT---\", verb)\n","\n","all_match = []\n","def descriptive_match_pattern(text):\n","    nlp = spacy.load('en_core_web_sm')\n","    matcher = Matcher(nlp.vocab)\n","    doc = nlp(text)\n","        \n","    #VERB + PREP + POBJ\n","    verb_dobj_patterns = [\n","        {'POS': 'VERB'},\n","        {'POS': 'DET', 'OP':'*'},\n","        {'DEP': 'dobj'}\n","    ]\n","\n","    verb_dobj_to_patterns = [\n","        {'POS': 'VERB'},\n","        {'POS': 'DET', 'OP':'*'},\n","        {'DEP': 'dobj'},\n","        {'DEP': 'prep'},\n","        {'POS': 'NOUN'}\n","    ]\n","\n","    matcher.add('TASK-EVENT', None, verb_dobj_patterns)\n","    matcher.add('TASK-EVENT', None, verb_dobj_to_patterns)\n","    matches = matcher(doc)\n","    \n","    for match in matches:\n","        match_id, start, end = match\n","        string_id = nlp.vocab.strings[match_id]\n","        span = doc[start:end]\n","        print(string_id, span.text)\n","        all_match.append(span.text)\n","\n","    my_match_list = list(dict.fromkeys(all_match))\n","    print(my_match_list)\n","\n","all_subs = [] \n","def find_left_and_right(doc):   \n","    index = 0\n","    nounIndices = []\n","\n","    noun_chunks_list = []\n","    for np in doc.noun_chunks: \n","        noun_chunks_list.append(str(np))\n","\n","    for token in doc:\n","        if token.pos_ == 'NOUN':\n","            nounIndices.append(index)\n","        index = index + 1\n","\n","    for idxValue in nounIndices:\n","        doc = nlp(text)\n","        span = doc[doc[idxValue].left_edge.i : doc[idxValue].right_edge.i+1]\n","        span.merge()\n","\n","        for token in doc:\n","            if token.pos_ == \"VERB\":\n","                subs = [tok for tok in token.rights if tok.dep_ in \"dobj\"]\n","                all_subs.append(token.text + \" \" +str(subs).replace('[','').replace(']',''))\n","    \n","    #TASK EVENT CLUSTER \n","    remove_repeately_list_with_dictionary()\n","\n","\n","def remove_repeately_list_with_dictionary():\n","    my_list = list(dict.fromkeys(all_subs))\n","    descriptive_match_pattern(str(my_list))\n","\n","#START\n","find_left_and_right(doc)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["TASK-EVENT sends notification\n","TASK-EVENT returns the good\n","['sends notification', 'returns the good']\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"tNuarGigzHeA","executionInfo":{"status":"ok","timestamp":1605425236073,"user_tz":-420,"elapsed":29559,"user":{"displayName":"Sut","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiqTtYUEpWQsymNl0gpoWkeQVgPIM3Wq21YCM51xtw=s64","userId":"11673919771648604082"}},"outputId":"359a2d29-79c2-4e45-b16e-38baa875fee5","colab":{"base_uri":"https://localhost:8080/"}},"source":["!pip install textacy"],"execution_count":5,"outputs":[{"output_type":"stream","text":["Collecting textacy\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/65/99/054efc5dea92c84a850639c490541de6cba29bc148debc3c73848c5e64c2/textacy-0.10.1-py3-none-any.whl (183kB)\n","\u001b[K     |████████████████████████████████| 184kB 6.1MB/s \n","\u001b[?25hCollecting pyphen>=0.9.4\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7c/5a/5bc036e01389bc6a6667a932bac3e388de6e7fa5777a6ff50e652f60ec79/Pyphen-0.10.0-py3-none-any.whl (1.9MB)\n","\u001b[K     |████████████████████████████████| 1.9MB 39.8MB/s \n","\u001b[?25hRequirement already satisfied: tqdm>=4.19.6 in /usr/local/lib/python3.6/dist-packages (from textacy) (4.41.1)\n","Requirement already satisfied: spacy<3.0.0,>=2.2.0 in /usr/local/lib/python3.6/dist-packages (from textacy) (2.2.4)\n","Requirement already satisfied: pyemd>=0.5.0 in /usr/local/lib/python3.6/dist-packages (from textacy) (0.5.1)\n","Requirement already satisfied: joblib>=0.13.0 in /usr/local/lib/python3.6/dist-packages (from textacy) (0.17.0)\n","Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from textacy) (1.4.1)\n","Requirement already satisfied: requests>=2.10.0 in /usr/local/lib/python3.6/dist-packages (from textacy) (2.23.0)\n","Requirement already satisfied: scikit-learn<0.24.0,>=0.19.0 in /usr/local/lib/python3.6/dist-packages (from textacy) (0.22.2.post1)\n","Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.6/dist-packages (from textacy) (2.5)\n","Requirement already satisfied: cachetools>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from textacy) (4.1.1)\n","Requirement already satisfied: srsly>=0.0.5 in /usr/local/lib/python3.6/dist-packages (from textacy) (1.0.2)\n","Collecting jellyfish>=0.7.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6c/09/927ae35fc5a9f70abb6cc2c27ee88fc48549f7bc4786c1d4b177c22e997d/jellyfish-0.8.2-cp36-cp36m-manylinux2014_x86_64.whl (93kB)\n","\u001b[K     |████████████████████████████████| 102kB 11.1MB/s \n","\u001b[?25hCollecting cytoolz>=0.8.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/58/67/1c60da8ba831bfefedb64c78b9f6820bdf58972797c95644ee3191daf27a/cytoolz-0.11.0.tar.gz (477kB)\n","\u001b[K     |████████████████████████████████| 481kB 44.8MB/s \n","\u001b[?25hRequirement already satisfied: numpy>=1.17.0 in /usr/local/lib/python3.6/dist-packages (from textacy) (1.18.5)\n","Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy<3.0.0,>=2.2.0->textacy) (0.4.1)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy<3.0.0,>=2.2.0->textacy) (50.3.2)\n","Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy<3.0.0,>=2.2.0->textacy) (1.0.0)\n","Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy<3.0.0,>=2.2.0->textacy) (1.1.3)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy<3.0.0,>=2.2.0->textacy) (3.0.2)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy<3.0.0,>=2.2.0->textacy) (1.0.3)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy<3.0.0,>=2.2.0->textacy) (2.0.4)\n","Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy<3.0.0,>=2.2.0->textacy) (0.8.0)\n","Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy<3.0.0,>=2.2.0->textacy) (7.4.0)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.10.0->textacy) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.10.0->textacy) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.10.0->textacy) (2020.6.20)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.10.0->textacy) (2.10)\n","Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx>=2.0->textacy) (4.4.2)\n","Requirement already satisfied: toolz>=0.8.0 in /usr/local/lib/python3.6/dist-packages (from cytoolz>=0.8.0->textacy) (0.11.1)\n","Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy<3.0.0,>=2.2.0->textacy) (2.0.0)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy<3.0.0,>=2.2.0->textacy) (3.4.0)\n","Building wheels for collected packages: cytoolz\n","  Building wheel for cytoolz (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for cytoolz: filename=cytoolz-0.11.0-cp36-cp36m-linux_x86_64.whl size=1225578 sha256=517b4f97d640380ebb1ac9114a401eb13151302470af2a979cb252a7aa08f9e7\n","  Stored in directory: /root/.cache/pip/wheels/a1/32/3c/9c9926b510647cacdde744b2c7acdf1ccd5896fbb7f8d5df0c\n","Successfully built cytoolz\n","Installing collected packages: pyphen, jellyfish, cytoolz, textacy\n","Successfully installed cytoolz-0.11.0 jellyfish-0.8.2 pyphen-0.10.0 textacy-0.10.1\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Ep1ueBYbzBga","executionInfo":{"status":"ok","timestamp":1605425241852,"user_tz":-420,"elapsed":3769,"user":{"displayName":"Sut","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiqTtYUEpWQsymNl0gpoWkeQVgPIM3Wq21YCM51xtw=s64","userId":"11673919771648604082"}},"outputId":"dea4851d-2667-499c-a0fd-b4c675704d40","colab":{"base_uri":"https://localhost:8080/","height":222}},"source":["import spacy\n","from spacy.symbols import nsubj, VERB\n","import textacy\n","\n","def findSubjectWithAncestor(text):\n","    nlp = spacy.load(\"en_core_web_sm\")\n","    doc = nlp(text)\n","    root = [token for token in doc if token.head == token][0]\n","    subject = list(root.lefts)[0]\n","    for descendant in subject.subtree:\n","        assert subject is descendant or subject.is_ancestor(descendant)\n","        print(descendant.text, descendant.dep_, descendant.n_lefts,\n","              descendant.n_rights,\n","              [ancestor.text for ancestor in descendant.ancestors])\n","\n","def findSubjectVerbsWithChildren(text):\n","    nlp = spacy.load(\"en_core_web_sm\")\n","    doc = nlp(text)\n","\n","    verbs = []\n","    for possible_verb in doc:\n","        print(possible_verb.tag_, possible_verb.text)\n","        if possible_verb.pos == VERB:\n","            for possible_subject in possible_verb.children:\n","                if possible_subject.dep == nsubj:\n","                    verbs.append(possible_verb)\n","                    break\n","    print(verbs)\n","\n","def findNounPhrasesVerbs(text):\n","    nlp = spacy.load(\"en_core_web_sm\")\n","    doc = nlp(text)\n","\n","    #VERB PHRASE\n","    patterns = [{\"POS\":\"ADV\"},{\"POS\":\"VERB\"}]\n","\n","    # Analyze syntax\n","    print(\"Noun phrases:\", [chunk.text for chunk in doc.noun_chunks])\n","    print(\"Verbs:\", [token.lemma_ for token in doc if token.pos_ == \"VERB\"])\n","\n","    # Find named entities, phrases and concepts\n","    for entity in doc.ents:\n","        print(entity.text, entity.label_)\n","\n","if __name__ == \"__main__\":\n","    findNounPhrasesVerbs(\"the sales department receives an order\")\n","    findSubjectVerbsWithChildren(\"the sales department receives an order\")"],"execution_count":6,"outputs":[{"output_type":"stream","text":["Noun phrases: ['the sales department', 'an order']\n","Verbs: ['receive']\n","DT the\n","NNS sales\n","NN department\n","VBZ receives\n","DT an\n","NN order\n","[receives]\n"],"name":"stdout"},{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["\"\\n(base) Toppees-MacBook-Pro:~ toppee$ python /Users/toppee/Desktop/test2.py\\nNoun phrases: ['we <NP>', 'a new order <NP>', 'the sales department <NP>', 'my masters <NP>', 'the necessary parts <NP>', 'quantities <NNS>', 'the delivery date <NP>']\\nVerbs: ['get <VBP>', 'determine <VBZ>']\\n(base) Toppees-MacBook-Pro:~ toppee$ \\n\""]},"metadata":{"tags":[]},"execution_count":6}]}]}